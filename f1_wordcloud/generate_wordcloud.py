# -*- coding: utf-8 -*-
"""generate_wordcloud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/KwonNayeon/medium-post-projects/blob/main/f1_wordcloud/generate_wordcloud.ipynb

# Deep Dive into Word Cloud Creation
This code links to the Medium post '*Deep Dive into Word Cloud Creation*'. Here, we'll show you how to create a word cloud using Python. The code covers data preparation and data visualization, focusing on various techniques for creating word clouds. By following along, you'll gain a better understanding of the technical aspects behind word cloud visualizations.

[Read the full article on Medium](https://medium.com/@nayeonkn0330/deep-dive-into-word-cloud-creation-c2fc7fc09c12)
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""## Required Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
import spacy
from PIL import Image
from matplotlib.colors import LinearSegmentedColormap
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

"""## Load data and basic information of the data (EDA)
Before getting into the word cloud creation, let's first take a look at the data. We'll start by loading the data and then explore its basic information, such as the data structure, data types, and any missing values. This initial exploratory data analysis (EDA) will help us understand the data and identify any preprocessing that may be needed.
"""

f1 = pd.read_csv("/content/gdrive/MyDrive/Colab Notebooks/F1/F1_tweets.csv")
f1.info()

f1.head()

"""#### Examine the Date Column in Detail
Let's take a closer look at the date column in our data. Limiting the data to the first half of 2022 will help us focus on the relevant information.
"""

# Check the minimum date
min_date = f1['date'].min()

# Check the maximum date
max_date = f1['date'].max()

# Display the results
print("Minimum date:", min_date)
print("Maximum date:", max_date)

# Identify and remove incorrect data in the date column
# Step 1: Convert the 'date' column to datetime format, forcing errors to identify bad data
f1['date'] = pd.to_datetime(f1['date'], errors='coerce')

# Step 2: Identify rows where the conversion failed (non-date values become NaT)
invalid_dates = f1[f1['date'].isna()]

# Display the rows with invalid date entries
print("Invalid date entries:", invalid_dates)

# Step 3: Drop rows with invalid dates, if necessary
f1 = f1.dropna(subset=['date'])

# Now, re-check the minimum and maximum dates
min_date = f1['date'].min()
max_date = f1['date'].max()

# Display the results
print("Minimum date:", min_date)
print("Maximum date:", max_date)

"""#### Count and Sort User Locations from F1 Tweets Dataset"""

f1['user_location'].value_counts()

location_counts = f1['user_location'].value_counts().reset_index()
location_counts.columns = ['user_location', 'count']
location_counts = location_counts[location_counts['user_location']!='NA']
location_counts = location_counts.sort_values(['count'],ascending=False)

# Display the DataFrame
print(location_counts)

"""## Pre-processing

#### Date Filtering (from Mar 01, 2022 to Jul 31, 2022)

This pre-processing step limits the analysis to the first half of the 2022 F1 season.
"""

# Define the start and end dates
start_date = '2022-03-01'
end_date = '2022-07-31'

# Filter the DataFrame and reassign it back to the original DataFrame
f1 = f1[(f1['date'] >= start_date) & (f1['date'] <= end_date)]

# Now f1 contains only the data within the specified date range

# Now, re-check the minimum and maximum dates
min_date = f1['date'].min()
max_date = f1['date'].max()

# Display the results
print("Minimum date:", min_date)
print("Maximum date:", max_date)

"""#### Aggregating Locations by Country

This code snippet performs several steps to clean and standardize the 'user_location' data, grouping it by countries or regions for more accurate analysis.




"""

# Ensure all values are strings
f1['user_location'] = f1['user_location'].astype(str).fillna('')

# Define a mapping dictionary for locations
location_mapping = {
    'United Kingdom': 'UK',
    'London, England': 'UK',
    'London': 'UK',
    'Walmer, Kent, UK': 'UK',
    'England, United Kingdom': 'UK',
    'England': 'UK',
    'Manchester': 'UK',
    'Essex': 'UK',
    'Paris': 'France',
    'Munster, Ireland': 'Ireland',
    'Worldwide': 'Worldwide',
    'Cape Town': 'South Africa',
    'Ireland': 'Ireland',
    'Miami': 'USA',
    'United States': 'USA',
    'BAKU': 'Azerbaijan',
    'Dallas, Texas': 'USA',
    'Los Angeles, CA': 'USA',
    'nan': 'Worldwide',
    'Oslo': 'Norway',
    'Mumbai': 'India'
}

# Apply the mapping to the 'user_location' column
f1['user_location'] = f1['user_location'].map(location_mapping).fillna(f1['user_location'])

# Define a function to aggregate locations using regex
def aggregate_locations(location):
    if isinstance(location, str):
        if re.search(r'London|England|UK|United Kingdom|Kent|Essex', location, re.IGNORECASE):
            return 'UK'
        elif re.search(r'Paris|France', location, re.IGNORECASE):
            return 'France'
        elif re.search(r'Ireland', location, re.IGNORECASE):
            return 'Ireland'
        elif re.search(r'Worldwide|nan', location, re.IGNORECASE):
            return 'Worldwide'
        elif re.search(r'Miami|United States|USA|Dallas, Texas|Los Angeles, CA', location, re.IGNORECASE):
            return 'USA'
        elif re.search(r'Cape Town', location, re.IGNORECASE):
            return 'South Africa'
        elif re.search(r'BAKU|Azerbaijan', location, re.IGNORECASE):
            return 'Azerbaijan'
        elif re.search(r'Oslo', location, re.IGNORECASE):
            return 'Norway'
        elif re.search(r'Mumbai', location, re.IGNORECASE):
            return 'India'
        else:
            return location
    else:
        return 'Unknown'  # Handle cases where location is not a string

# Apply the function to the 'user_location' column
f1['user_location'] = f1['user_location'].apply(aggregate_locations)

location_counts = f1['user_location'].value_counts().reset_index()
location_counts.columns = ['user_location', 'count']
location_counts = location_counts[location_counts['user_location']!='NA']
location_counts = location_counts.sort_values(['count'],ascending=False)

# Display the DataFrame
location_counts.head(10)

"""## Data Visualization

### Bar plot of location distribution after applying location mapping
"""

# Plot the top 10 user locations as a horizontal bar chart
plt.figure(figsize=(12, 8))  # Set the figure size
sns.barplot(
    x='count', y='user_location', data=location_counts[:10],
    palette='Blues_d'  # Use a blue color palette for the bars
)

# Add labels to the plot
plt.xlabel('Tweet Count')  # Label for the x-axis
plt.ylabel('User Location')  # Label for the y-axis
plt.title('Top 10 User Locations by Tweet Count')  # Title of the plot
plt.grid(True, axis='x', linestyle='--', alpha=0.7)  # Add gridlines for readability

# Add count values on the bars
for index, value in enumerate(location_counts['count'][:10]):
    plt.text(value, index, f'{value}', va='center')  # Display count values on the bars

# Save the plot as a PNG file before showing it
plt.savefig('user_location_plot_edited.png', format='png', bbox_inches='tight', dpi=300)  # Save with high resolution

plt.show()  # Display the plot

"""### Word Cloud

#### Word Cloud of F1 Tweets (Basic, Without User Tuning)
"""

# Ensure that the 'text' column contains strings and handle NaN values
f1['text'] = f1['text'].astype(str).fillna('')

# Combine all text data into a single string
all_text = ' '.join(f1['text'])

# Generate a basic word cloud
wordcloud = WordCloud(
    width=800,  # Set the width of the word cloud
    height=400,  # Set the height of the word cloud
    background_color='white'  # Set the background color
).generate(all_text)

# Display the word cloud
plt.figure(figsize=(10, 5))  # Adjust the figure size
plt.imshow(wordcloud, interpolation='bilinear')  # Smooth rendering
plt.axis('off')  # Hide the axis
plt.title('Basic Word Cloud')  # Add a title
plt.show()  # Render the plot

"""#### Basic Word Cloud of F1 Tweets with Custom Stopwords


"""

# Define custom stopwords
custom_stopwords = set(STOPWORDS).union({
    'RT', 'https', 'the', 'and', 'of', 'to', 'will', 'much', 'see', 'now', 'seen',
    'come', 'know', 'haa', 'football club', 'even', 't', 'co', 'f1oki f1okidao',
    'far', 'take', 'don', 'f1okidao', 'f1okidao doge', 'thing',
    'got', 'really', 's', 'u', 'still', 'way', 'f1'
})

# Combine all text data into a single string without changing case
all_text = ' '.join(f1['text'].fillna(''))

# Remove unnecessary variations and normalize terms like "F1"
all_text = re.sub(r'\bf1\b', 'F1', all_text, flags=re.IGNORECASE)  # Replace "f1" with "F1"
all_text = re.sub(r'#f1\b', 'F1', all_text, flags=re.IGNORECASE)  # Replace #f1 with F1
all_text = re.sub(r'\bf1gp\b', 'F1', all_text, flags=re.IGNORECASE)  # Replace f1gp with F1

# Generate the word cloud using the custom stopwords and colormap
wordcloud = WordCloud(
    width=800,
    height=400,
    stopwords=custom_stopwords,  # Apply custom stopwords
    background_color='white',  # White background for contrast
    colormap='viridis'  # Custom colormap
).generate(all_text)

# Display the word cloud
plt.figure(figsize=(10, 6))  # Adjust the figure size for better display
plt.imshow(wordcloud, interpolation='bilinear')  # Smooth rendering
plt.axis('off')  # Hide the axis
plt.title('Word Cloud of F1 Tweets with Custom Stopwords')  # Add a title
plt.show()  # Render the plot

"""#### Word Cloud of F1 Tweets with Custom Colormap and Custom Stopwords"""

# Define an F1-themed colormap
f1_theme_cmap = LinearSegmentedColormap.from_list('f1_theme', ['#00D2BE', '#000000', '#C0C0C0'])  # teal, black, silver
f1_theme_cmap_r = LinearSegmentedColormap.from_list('f1_theme_r', ['#FF0000', '#FFFFFF', '#000000'])  # red, white, black

# Generate the word cloud using the custom colormap and stopwords
wordcloud = WordCloud(
    width=800,
    height=400,
    stopwords=custom_stopwords,  # Apply custom stopwords
    background_color='white',  # White background for contrast
    colormap=f1_theme_cmap  # Use the custom colormap
).generate(all_text)

# Display the word cloud
plt.figure(figsize=(10, 5))  # Adjust figure size for better display
plt.imshow(wordcloud, interpolation='bilinear')  # Smooth rendering
plt.axis('off')  # Hide the axis
plt.title('Word Cloud of F1 Tweets with Custom Colormap and Stopwords')  # Add a title
plt.show()  # Render the plot

"""#### Enhanced F1 Tweet Word Cloud: Custom Stopwords, Colormap, and Mask Image"""

# Define the path to the mask image
mask_path = '/content/gdrive/MyDrive/Colab Notebooks/F1/racing.png'  # Update this path to your mask image location

# Load the mask image
mask_image = np.array(Image.open(mask_path))

# Generate the word cloud with the F1 colormap
wordcloud = WordCloud(
    width=mask_image.shape[1],  # Match the mask image width
    height=mask_image.shape[0],  # Match the mask image height
    background_color='white',  # Light background for contrast
    colormap=f1_theme_cmap,  # Use a predefined colormap name
    stopwords=custom_stopwords,
    mask=mask_image
).generate(all_text)

# Display the word cloud
plt.figure(figsize=(11.25, 6.25))  # Adjust figure size for better display
plt.imshow(wordcloud, interpolation='bilinear')  # Smooth rendering
plt.axis('off')  # Hide the axis
plt.title('Enhanced F1 Tweet Word Cloud: Custom Stopwords, Colormap, and Mask Image')  # Add a title
plt.show()  # Render the plot